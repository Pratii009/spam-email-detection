{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5c72758",
   "metadata": {},
   "source": [
    "NLP TECHNIQUE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2404f7a6",
   "metadata": {},
   "source": [
    "TEXT PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9a3cf9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "98326331",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\prati\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\prati\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\prati\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fa02cca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Text:\n",
      "hello my name is pratii and i am a btech student\n"
     ]
    }
   ],
   "source": [
    "text=\"Hello my name is, Pratii! and I am, a btech student\"\n",
    "text=text.lower()\n",
    "text=re.sub(r'[^\\w\\s]','',text)\n",
    "print(\"Normalized Text:\")\n",
    "print (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6925ea46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens:\n",
      "['Hello', 'my', 'name', 'is', 'Pratii', 'and', 'I', 'am', 'a', 'btech', 'student', 'and', 'i', 'love', 'singing']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\prati\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "text=\"Hello my name is, Pratii! and I am, a btech student.  and i love singing\"\n",
    "text=re.sub(r'[^\\w\\s]','',text)\n",
    "tokens=word_tokenize(text)\n",
    "print(\"\\nTokens:\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1ee29147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens:\n",
      "['Hello', 'my', 'name', 'is', 'Pratii', 'and', 'I', 'am', 'a', 'btech', 'student', 'and', 'i', 'love', 'singing']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text=\"Hello my. and name is, Pratii! and I am  a btech student. and i love singing \"\n",
    "tokens1=sent_tokenize(text)\n",
    "print(\"\\nTokens:\")\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0eff68bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Stopword Removal:\n",
      "['Hello', 'name', 'Pratii', 'I', 'btech', 'student', 'love', 'singing']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\prati\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "#text=\"Hello my. and name is, Pratii! and I am a btech student. and i love singing\"\n",
    "stop_words=set(stopwords.words('english'))\n",
    "filtered_tokens=[word for word in tokens if word not in stop_words]\n",
    "print(\"\\nAfter Stopword Removal:\")\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "930231e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Stemming\n",
      "['hello', 'name', 'pratii', 'i', 'btech', 'student', 'love', 'sing']\n"
     ]
    }
   ],
   "source": [
    "stemmer= PorterStemmer()\n",
    "stemmed_words=[stemmer.stem(word) for word in filtered_tokens]\n",
    "print(\"\\nAfter Stemming\")\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7df720ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Lemmatization:\n",
      "['Hello', 'name', 'Pratii', 'I', 'btech', 'student', 'love', 'singing']\n"
     ]
    }
   ],
   "source": [
    "lematizer=WordNetLemmatizer()\n",
    "lemmatized_words=[lematizer.lemmatize(word) for word in filtered_tokens]\n",
    "print(\"\\nAfter Lemmatization:\")\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7f6b0323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagging:\n",
      "the -> DET\n",
      "little -> ADJ\n",
      "boy -> NOUN\n",
      "is -> AUX\n",
      "playing -> VERB\n",
      "football -> NOUN\n",
      "in -> ADP\n",
      "the -> DET\n",
      "park -> NOUN\n",
      ". -> PUNCT\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "sentence=\"the little boy is playing football in the park.\"\n",
    "doc=nlp(sentence)\n",
    "print(\"POS Tagging:\")\n",
    "for token in doc:\n",
    "    print(token.text,\"->\",token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "82dd0622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dependency Parsing\n",
      "the->det->boy\n",
      "little->amod->boy\n",
      "boy->nsubj->playing\n",
      "is->aux->playing\n",
      "playing->ROOT->playing\n",
      "football->dobj->playing\n",
      "in->prep->playing\n",
      "the->det->park\n",
      "park->pobj->in\n",
      ".->punct->playing\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDependency Parsing\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text}->{token.dep_}->{token.head.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5b97c8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Constituency Parseing(Noun Phrases):\n",
      "the little boy -> the little boy\n",
      "football -> football\n",
      "the park -> the park\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Constituency Parseing(Noun Phrases):\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text,\"->\",chunk.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf66b91",
   "metadata": {},
   "source": [
    "semantic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e9ac11d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entitled:\n",
      "Alice -> PERSON\n",
      "Google -> ORG\n",
      "Chennai -> GPE\n",
      "2021 -> DATE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "text=\"Alice  works at Google in Chennai.He joined the company in 2021.\"\n",
    "doc=nlp(text)\n",
    "print(\"Named Entitled:\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text,\"->\",ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1180b64c",
   "metadata": {},
   "source": [
    "text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "475a8e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Scores:\n",
      "{'neg': 0.0, 'neu': 0.711, 'pos': 0.289, 'compound': 0.431}\n",
      "Sentiment : Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\prati\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "#text=\"I really did not love this product.It does not works!\"\n",
    "text=\"not a bad product nor better,it is only okok \"\n",
    "sia=SentimentIntensityAnalyzer()\n",
    "sentiment=sia.polarity_scores(text)\n",
    "print(\"Sentiment Scores:\")\n",
    "print(sentiment)\n",
    "\n",
    "if sentiment['compound']>=0.05:\n",
    "    print(\"Sentiment : Positive\")\n",
    "elif sentiment[\"compound\"]<=-0.05:\n",
    "    print(\"Sentiment : Negative\")\n",
    "else:\n",
    "    print(\"Sentiment : Neutral\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1c42ac",
   "metadata": {},
   "source": [
    "spam email classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8246ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message:  meet me tommorown to won free money claim \n",
      "Result : Spam Messages❌ \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "messages=[\"win money now\",\n",
    "          \"Congratulations you won a prize\",\n",
    "          \"Free entry in a contest\",\n",
    "          \"claim your free reward\",\n",
    "          \"Hey, how are you\",\n",
    "          \"Let's meet tommorow\",\n",
    "          \"Are you coming to class\",\n",
    "          \"Call me when free\"]\n",
    "\n",
    "# labels: 1=spam, 0=not spam\n",
    "labels=[1,1,1,1,0,0,0,0]\n",
    "\n",
    "vectorizer=CountVectorizer()\n",
    "x=vectorizer.fit_transform(messages)\n",
    "\n",
    "model=MultinomialNB()\n",
    "model.fit(x,labels)\n",
    "\n",
    "test_message=[\" meet me tommorown to won free money claim \"]\n",
    "test_vector=vectorizer.transform(test_message)\n",
    "\n",
    "prediction=model.predict(test_vector)\n",
    "\n",
    "\n",
    "print(\"message:\",test_message[0])\n",
    "\n",
    "if prediction[0]==1:\n",
    "  print(\"Result : Spam Messages❌ \")\n",
    "else:\n",
    "  print(\"Result: Not Spam ✅\")\n",
    "\n",
    "\n",
    "  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
